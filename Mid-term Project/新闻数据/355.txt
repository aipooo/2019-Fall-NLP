
原标题：PyTorch&TensorFlow跑分对决：哪个平台运行NLP模型推理更快  来源：量子位
关注前沿科技
                                                    
                                                                
                                        
                      
                        量子位
然而还有一项不可忽略的因素，就是二者的实际性能。
没关系，不服跑个分？！
最近，一位来自“Huggingface”的工程师，使用了NLP中的Transformer模型，分别在两大平台上测试了一组推理速度。
虽然Huggingface只是一家创业公司，但是在NLP领域有着不小的声誉，他们在GitHub上开源的项目，只需一个API就能调用27个NLP模型广受好评，已经收获1.5万星。
PyTorch和TensorFlow究竟哪个更快？下面用详细评测的数据告诉你。
运行环境
作者在PyTorch 1.3.0、TenserFlow2.0上分别对CPU和GPU的推理性能进行了测试。
两种不同的环境中具体硬件配置如下：
CPU推理：使用谷歌云平台上的n1-standard-32硬件，即32个vCPU、120GB内存，CPU型号为2.3GHz的英特尔至强处理器。
GPU推理：使用谷歌云平台上的定制化硬件，包含12个vCPU、40GB内存和单个V100 GPU（16GB显存）。
在测试过程中使用本地Python模块的timeit来测量推理时间。每个实验重复30次，然后对这30个值取平均值，获得平均推理时间。
NLP模型的Batch Size设置为分别设置为1、2、4、8，序列长度为8、64,、128、256、512、1024。
测试结果
话不多说，先上跑分结果：
在大多数情况下，这两个平台都能获得相似的结果。与PyTorch相比，TensorFlow在CPU上通常要慢一些，但在GPU上要快一些：
在CPU上，PyTorch的平均推理时间为0.748s，而TensorFlow的平均推理时间为0.823s。
在GPU上，PyTorch的平均推理时间为0.046s，而TensorFlow的平均推理时间为0.043s。
以上的数据都是在所有模型总的平均结果。结果显示，输入大小（Batch Size×序列长度）越大，对最终结果的影响也越大。
当输入太大时，PyTorch会出现内存不足的情况。作者把这些部分从结果中删除，因此这会使结果偏向PyTorch。
总的来说，PyTorch模型比TensorFlow模型更容易耗尽内存。除了Distilled模型之外，当输入大小达到8的Batch Size和1024的序列长度时，PyTorch就会耗尽内存。
至于更完整详细的清单，请参阅文末的Google文档链接。
两大平台的加速工具
除了初步的测试，作者还用上两个平台独有的加速工具，看看它们对模型推理速度有多大的提升。
TorchScript是PyTorch创建可序列化模型的方法，让模型可以在不同的环境中运行，而无需Python依赖项，例如C++环境。
TorchScript似乎非常依赖于模型和输入大小：
使用TorchScript可以在XLNet上产生永久的性能提升，而在XLM上使用则会不可靠；
在XLM上，TorchScript可以提高较小输入时的性能，但会降低较大输入时的性能。
平均而言，使用TorchScript跟踪的模型，推理速度要比使用相同PyTorch非跟踪模型的快20％。
XLA是可加速TensorFlow模型的线性代数编译器。作者仅在基于TensorFlow的自动聚类功能的GPU上使用它，这项功能可编译一些模型的子图。结果显示：
启用XLA提高了速度和内存使用率，所有模型的性能都有提高。
大多数基准测试的运行速度提升到原来的1.15倍。在某些极端情况下，推理时间减少了70％，尤其是在输入较小的情况下。
最后，作者还在Google文档的列表里还加入了“训练”选项卡，或许不久后就能看到两大平台上的训练测试对比，唯一挡在这项测试面前的障碍可能就是经费了。
传送门
（声明：本文仅代表作者观点，不代表新浪网立场。）
